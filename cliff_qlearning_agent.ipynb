{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63ee02f6-75bf-4bb3-bce7-ae52f64262be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class CliffWalkingAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "        visualize_training: bool = False,\n",
    "        render_frequency: int = 1 # nur bei visualize_training=True relevant\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent für CliffWalking-v1.\n",
    "        Reward: Each time step incurs -1 reward, unless the player stepped into the cliff, which incurs -100 reward.\n",
    "        \"\"\"\n",
    "        # Initialisierungs Parameter - Umgebung\n",
    "\n",
    "        self.render_env = env\n",
    "\n",
    "        # Wenn Visualisierung nicht gewünscht ist, erstelle eine Trainingsumgebung ohne render_mode\n",
    "        # ansonsten verwende die übergebene Umgebung für Training und Rendern\n",
    "        if not visualize_training:\n",
    "            try:\n",
    "                self.train_env = gym.make(self.render_env.spec.id)\n",
    "            except Exception:\n",
    "                self.train_env = self.render_env\n",
    "        else:\n",
    "            # Visualisierung gewünscht: trainiere direkt in der gegebenen Umgebung\n",
    "            self.train_env = self.render_env\n",
    "\n",
    "        self.env = self.train_env\n",
    "\n",
    "        # Intialisierung - Q-Table und Parameter\n",
    "\n",
    "        # Q-Table\n",
    "        self.q_values = defaultdict(lambda: np.zeros(self.train_env.action_space.n)) # hier noch mal anschauen\n",
    "\n",
    "        # Parameter\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        # Epsilon-Greedy\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        # Tracking\n",
    "        self.training_error = []\n",
    "        self.return_queue = []\n",
    "        self.length_queue = []\n",
    "\n",
    "        # Visualisierung\n",
    "        self.visualize_training = visualize_training\n",
    "        self.render_frequency = render_frequency\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def get_action(self, state: int) -> int:\n",
    "        \"\"\"Epsilon-Greedy Aktionswahl.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Verwende die Trainingsumgebung zum Sample, damit kein Render-Fenster erzeugt wird\n",
    "            return self.train_env.action_space.sample()\n",
    "        return int(np.argmax(self.q_values[state]))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def update(self, state, action, reward, terminated, next_state):\n",
    "        \"\"\"\n",
    "        Q-Learning Update mit Bellman-Gleichung.\n",
    "        \"\"\"\n",
    "        future_q = 0 if terminated else np.max(self.q_values[next_state])\n",
    "\n",
    "        target = reward + self.discount_factor * future_q\n",
    "        temporal_diff = target - self.q_values[state][action]\n",
    "\n",
    "        self.q_values[state][action] += self.lr * temporal_diff\n",
    "\n",
    "        self.training_error.append(temporal_diff)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Reduzierung von Epsilon pro Episode.\"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def train_q_learning(self, episodes: int, max_steps: int = 200):\n",
    "        \"\"\"\n",
    "        Training mit optionaler Visualisierung über rgb_array.\n",
    "        Gibt bei visualize_training=True die gesammelten Frames zurück.\n",
    "        \"\"\"\n",
    "        frames = []  # Sammlung für Jupyter-Animation\n",
    "    \n",
    "        for ep in range(episodes):\n",
    "            state, _ = self.train_env.reset()\n",
    "    \n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "    \n",
    "            for step in range(max_steps):\n",
    "    \n",
    "                # Frame speichern (nur wenn visualisierung eingeschaltet & Frequenz passt)\n",
    "                if self.visualize_training and ep % self.render_frequency == 0:\n",
    "                    try:\n",
    "                        frame = self.render_env.render()\n",
    "                        frames.append(frame)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    \n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.train_env.step(action)\n",
    "    \n",
    "                done = terminated or truncated\n",
    "    \n",
    "                # Q-Learning Update\n",
    "                self.update(state, action, reward, done, next_state)\n",
    "    \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "    \n",
    "                if done:\n",
    "                    break\n",
    "    \n",
    "            # Tracking\n",
    "            self.return_queue.append(total_reward)\n",
    "            self.length_queue.append(steps)\n",
    "            self.decay_epsilon()\n",
    "    \n",
    "            if not self.visualize_training and ep % 100 == 0:\n",
    "                print(f\"[Episode {ep}] Reward={total_reward}, Steps={steps}, ε={self.epsilon:.3f}\")\n",
    "    \n",
    "        print(\"Training abgeschlossen!\\n\")\n",
    "    \n",
    "        # Frames zurückgeben\n",
    "        if self.visualize_training:\n",
    "            return frames\n",
    "        return None\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    def run_policy_demo(self, max_steps=200):\n",
    "        \"\"\"\n",
    "        Führt eine Demo-Episode aus und liefert Frames im rgb_array-Format.\n",
    "        Ideal für Jupyter Notebook Animation.\n",
    "        \"\"\"\n",
    "        print(\"\\nStarte Demo-Episode basierend auf gelernten Q-Werten ...\\n\")\n",
    "    \n",
    "        # Demo-Umgebung immer mit rgb_array erzeugen\n",
    "        demo_env = gym.make(self.render_env.spec.id, render_mode=\"rgb_array\")\n",
    "    \n",
    "        state, _ = demo_env.reset()\n",
    "    \n",
    "        frames = []\n",
    "        try:\n",
    "            frames.append(demo_env.render())\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        total_reward = 0\n",
    "    \n",
    "        for _ in range(max_steps):\n",
    "            action = np.argmax(self.q_values[state])\n",
    "            next_state, reward, terminated, truncated, _ = demo_env.step(action)\n",
    "    \n",
    "            # Frame speichern\n",
    "            try:\n",
    "                frame = demo_env.render()\n",
    "                frames.append(frame)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "    \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "    \n",
    "        print(f\"Demo abgeschlossen! Total Reward: {total_reward}\")\n",
    "    \n",
    "        return frames\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    def get_q_table_df(self):\n",
    "        \"\"\"Gibt die Q-Table als pandas DataFrame zurück.\"\"\"\n",
    "        df_q = pd.DataFrame(\n",
    "            {state: self.q_values[state] for state in self.q_values}\n",
    "        ).T\n",
    "        df_q.columns = [f\"action_{i}\" for i in range(df_q.shape[1])]\n",
    "        df_q.index.name = \"state\"\n",
    "        \n",
    "        return df_q"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
